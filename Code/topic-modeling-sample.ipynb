{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import snscrape.modules.twitter as tw\n",
    "\n",
    "# f = open('em.txt','w',encoding='utf-8')\n",
    "\n",
    "# #Save Twitter data to external file\n",
    "# #Possible outputs: url, date, content, id, username, outlinks, outlinksss, tcooutlinks, tcooutlinksss\n",
    "# for tweet in tw.TwitterSearchScraper(query=\"(from:elonmusk) since:2020-08-01 until:2020-11-01\").get_items():\n",
    "#     date_str = tweet.date.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "#     date_str = date_str[:-2] + \":\" + date_str[-2:]\n",
    "#     #f.write(date_str + \"|\" + tweet.content + \"\\n\")\n",
    "#     f.write(date_str + \"|\" + tweet.rawContent + \"\\n\")\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Time  \\\n",
      "0   2020-10-31 19:34:19-04:00   \n",
      "1   2020-10-31 18:09:40-04:00   \n",
      "2   2020-10-31 18:04:30-04:00   \n",
      "3   2020-10-31 18:00:41-04:00   \n",
      "4   2020-10-31 17:59:44-04:00   \n",
      "..                        ...   \n",
      "760 2020-08-02 00:49:11-04:00   \n",
      "761 2020-08-01 04:15:54-04:00   \n",
      "762 2020-08-01 02:47:18-04:00   \n",
      "763 2020-07-31 23:57:37-04:00   \n",
      "764 2020-07-31 23:51:54-04:00   \n",
      "\n",
      "                                                 Tweet        Date  \n",
      "0                       @RGVaerialphotos Lord of the â€¦  2020-10-31  \n",
      "1    @TGMetsFan98 @KlotzAdam @NASASpaceflight @Erda...  2020-10-31  \n",
      "2      @flcnhvy @NASASpaceflight @Erdayastronaut Minor  2020-10-31  \n",
      "3    @NASASpaceflight @Erdayastronaut But, a RUD ri...  2020-10-31  \n",
      "4    @NASASpaceflight @Erdayastronaut Stable, contr...  2020-10-31  \n",
      "..                                                 ...         ...  \n",
      "760  @meier1028 @SpaceX @Space_Station @AstroBehnke...  2020-08-02  \n",
      "761  @engineeringvids The simplest solution is not ...  2020-08-01  \n",
      "762                                               ðŸ§™â€â™‚ï¸  2020-08-01  \n",
      "763  @cybrtrck Absolutely. Long-lasting art is incr...  2020-07-31  \n",
      "764  This BBC article provides a sensible summary f...  2020-07-31  \n",
      "\n",
      "[765 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "import pytz\n",
    "\n",
    "#Read Twitter data into Python\n",
    "em = []\n",
    "dates = []\n",
    "f = open(\"em.txt\", \"r\", encoding=\"utf-8\")\n",
    "for l in f:\n",
    "    line = l.split(\"|\")\n",
    "    date_str = line[0]#+\"+00:00\"\n",
    "    try:\n",
    "        date_time = dt.fromisoformat(date_str)\n",
    "        date_time = date_time.astimezone(pytz.timezone(\"US/Eastern\"))\n",
    "        line[0] = date_time\n",
    "        line[1] = line[1][:-1]\n",
    "        em.append(line)\n",
    "        dates.append(date_time.date())\n",
    "    except:\n",
    "        em[-1][1] += \" \"+l[:-1]\n",
    "f.close()\n",
    "\n",
    "em = pd.DataFrame(data=em , columns=['Time','Tweet'])\n",
    "em['Date'] = dates\n",
    "#em['Date'] = em['Date'].astype(str)\n",
    "print(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process for Latent Dirichlet Allocation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Initialize regex tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # Tokenizes by word\n",
    "\n",
    "# Vectorize in one of 2 ways:\n",
    "# Term frequency:\n",
    "tf = CountVectorizer(lowercase=True,stop_words='english',ngram_range=(1,1),tokenizer=tokenizer.tokenize)\n",
    "tf_data = tf.fit_transform(em.Tweet.tolist())\n",
    "tf_feature_names = tf.get_feature_names_out()\n",
    "\n",
    "# Term frequency-inverse document frequency\n",
    "tfidf = TfidfVectorizer(lowercase=True,stop_words='english',ngram_range=(1,1),tokenizer=tokenizer.tokenize)\n",
    "tfidf_data = tfidf.fit_transform(em.Tweet.tolist())\n",
    "tfidf_feature_names = tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Latent Dirichlet Allocation\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "K = 5 # Number of topics\n",
    "\n",
    "# Try with term frequency:\n",
    "LDA_TF = LatentDirichletAllocation(n_components=K)\n",
    "LDA_TF_Matrix = LDA_TF.fit_transform(tf_data) # Fits and transforms the dataset\n",
    "LDA_TF_Components = LDA_TF.components_ # Get the components\n",
    "\n",
    "# Try with term frequency-inverse document frequency:\n",
    "LDA_TFIDF = LatentDirichletAllocation(n_components=K)\n",
    "LDA_TFIDF_Matrix = LDA_TFIDF.fit_transform(tfidf_data) # Fits and transforms the dataset\n",
    "LDA_TFIDF_Components = LDA_TFIDF.components_ # Get the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency\n",
      "Topic 1:  ['ppathole', 'tesla', 'flcnhvy', 's', 'yes', 'teslaownerssv', 'spacex', 'amp', 'evafoxu', 'true']\n",
      "Topic 2:  ['amp', 'erdayastronaut', 'flcnhvy', 'spacex', 'ppathole', 's', 'wholemarsblog', 'yes', 'tesla', 'kristennetten']\n",
      "Topic 3:  ['t', 'https', 'tesla', 'yes', 'amp', 'erdayastronaut', 'wholemarsblog', 'spacex', 'teslarati', 'waitbutwhy']\n",
      "Topic 4:  ['t', 'https', 's', 'tesla', 'spacex', 'amp', 'ppathole', 'great', 'teslaownerssv', 'wholemarsblog']\n",
      "Topic 5:  ['amp', 'nasaspaceflight', 'spacex', 'erdayastronaut', 's', 'tesla', 'production', 'high', 'marcushousegame', 'flcnhvy']\n",
      " \n",
      "Term Frequency-Inverse Document Frequency\n",
      "Topic 1:  ['erdayastronaut', 'amp', 'spacex', 't', 'cleantechnica', 'ppathole', 'https', 'need', 'flcnhvy', 's']\n",
      "Topic 2:  ['ppathole', 'tesla', 'amp', 'teslaownerssv', 'soon', 't', 'wholemarsblog', 'coming', 'https', 'sure']\n",
      "Topic 3:  ['t', 'https', 'evafoxu', 'great', 'spacex', 's', 'pretty', 'years', 'teslarati', 'neuralink']\n",
      "Topic 4:  ['yes', 'wholemarsblog', 'haha', 't', 'tesla', 'amp', 'flcnhvy', 'https', 'erdayastronaut', 'ppathole']\n",
      "Topic 5:  ['amp', 'erdayastronaut', 'sure', 'flcnhvy', 'spacex', 'berniesanders', 'exactly', 's', 'tesla', 'ppathole']\n"
     ]
    }
   ],
   "source": [
    "# Displaying Topics\n",
    "def display_topics(components, feature_names, no_top_words):\n",
    "    for index, topic in enumerate(components):\n",
    "        top_terms_list = [feature_names[i] for i in topic.argsort()[:-no_top_words-1:-1]]\n",
    "        print(\"Topic \"+str(index+1)+\": \",top_terms_list)\n",
    "\n",
    "no_top_words = 10\n",
    "print(\"Term Frequency\")\n",
    "display_topics(LDA_TF_Components , tf_feature_names , no_top_words)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Term Frequency-Inverse Document Frequency\")\n",
    "display_topics(LDA_TFIDF_Components , tfidf_feature_names , no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6676496  0.0829821  0.08298097 0.08298015 0.08340717]\n",
      " [0.0457915  0.04477599 0.04428458 0.04458284 0.82056508]\n",
      " [0.06942209 0.0685405  0.06868892 0.06958301 0.72376548]\n",
      " ...\n",
      " [0.2        0.2        0.2        0.2        0.2       ]\n",
      " [0.77798866 0.05642555 0.05515358 0.05520521 0.05522701]\n",
      " [0.05370261 0.05383614 0.78441895 0.05372717 0.05431514]]\n",
      "                         Time  \\\n",
      "0   2020-10-31 19:34:19-04:00   \n",
      "1   2020-10-31 18:09:40-04:00   \n",
      "2   2020-10-31 18:04:30-04:00   \n",
      "3   2020-10-31 18:00:41-04:00   \n",
      "4   2020-10-31 17:59:44-04:00   \n",
      "..                        ...   \n",
      "760 2020-08-02 00:49:11-04:00   \n",
      "761 2020-08-01 04:15:54-04:00   \n",
      "762 2020-08-01 02:47:18-04:00   \n",
      "763 2020-07-31 23:57:37-04:00   \n",
      "764 2020-07-31 23:51:54-04:00   \n",
      "\n",
      "                                                 Tweet        Date  Cluster  \n",
      "0                       @RGVaerialphotos Lord of the â€¦  2020-10-31        0  \n",
      "1    @TGMetsFan98 @KlotzAdam @NASASpaceflight @Erda...  2020-10-31        4  \n",
      "2      @flcnhvy @NASASpaceflight @Erdayastronaut Minor  2020-10-31        4  \n",
      "3    @NASASpaceflight @Erdayastronaut But, a RUD ri...  2020-10-31        4  \n",
      "4    @NASASpaceflight @Erdayastronaut Stable, contr...  2020-10-31        4  \n",
      "..                                                 ...         ...      ...  \n",
      "760  @meier1028 @SpaceX @Space_Station @AstroBehnke...  2020-08-02        2  \n",
      "761  @engineeringvids The simplest solution is not ...  2020-08-01        2  \n",
      "762                                               ðŸ§™â€â™‚ï¸  2020-08-01        0  \n",
      "763  @cybrtrck Absolutely. Long-lasting art is incr...  2020-07-31        0  \n",
      "764  This BBC article provides a sensible summary f...  2020-07-31        2  \n",
      "\n",
      "[765 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Classifying the Documents\n",
    "# Let's just look at TF-IDF\n",
    "print(LDA_TFIDF_Matrix)\n",
    "\n",
    "label = np.argmax(LDA_TFIDF_Matrix , axis=1)\n",
    "em['Cluster'] = label\n",
    "print(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Time  \\\n",
      "0   2020-10-31 19:34:19-04:00   \n",
      "1   2020-10-31 18:09:40-04:00   \n",
      "2   2020-10-31 18:04:30-04:00   \n",
      "3   2020-10-31 18:00:41-04:00   \n",
      "4   2020-10-31 17:59:44-04:00   \n",
      "..                        ...   \n",
      "760 2020-08-02 00:49:11-04:00   \n",
      "761 2020-08-01 04:15:54-04:00   \n",
      "762 2020-08-01 02:47:18-04:00   \n",
      "763 2020-07-31 23:57:37-04:00   \n",
      "764 2020-07-31 23:51:54-04:00   \n",
      "\n",
      "                                                 Tweet        Date  Cluster  \\\n",
      "0                       @RGVaerialphotos Lord of the â€¦  2020-10-31        0   \n",
      "1    @TGMetsFan98 @KlotzAdam @NASASpaceflight @Erda...  2020-10-31        4   \n",
      "2      @flcnhvy @NASASpaceflight @Erdayastronaut Minor  2020-10-31        4   \n",
      "3    @NASASpaceflight @Erdayastronaut But, a RUD ri...  2020-10-31        4   \n",
      "4    @NASASpaceflight @Erdayastronaut Stable, contr...  2020-10-31        4   \n",
      "..                                                 ...         ...      ...   \n",
      "760  @meier1028 @SpaceX @Space_Station @AstroBehnke...  2020-08-02        2   \n",
      "761  @engineeringvids The simplest solution is not ...  2020-08-01        2   \n",
      "762                                               ðŸ§™â€â™‚ï¸  2020-08-01        0   \n",
      "763  @cybrtrck Absolutely. Long-lasting art is incr...  2020-07-31        0   \n",
      "764  This BBC article provides a sensible summary f...  2020-07-31        2   \n",
      "\n",
      "     Sentiment  \n",
      "0       0.0000  \n",
      "1      -0.1531  \n",
      "2       0.0000  \n",
      "3       0.4235  \n",
      "4       0.8858  \n",
      "..         ...  \n",
      "760     0.6249  \n",
      "761     0.6801  \n",
      "762     0.0000  \n",
      "763    -0.4201  \n",
      "764     0.0000  \n",
      "\n",
      "[765 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#The compound score is computed by summing the valence scores of each word in the lexicon, \n",
    "#adjusted according to the rules, and then normalized to be between -1 (most extreme negative) \n",
    "#and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional \n",
    "#measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n",
    "#It is also useful for researchers who would like to set standardized thresholds for classifying sentences \n",
    "#as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:\n",
    "#    positive sentiment: compound score >= 0.05\n",
    "#    neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "#    negative sentiment: compound score <= -0.05\n",
    "#The pos, neu, and neg scores are ratios for proportions of text that fall in each category (so these should all \n",
    "#add up to be 1... or close to it with float operation). These are the most useful metrics if you want \n",
    "#multidimensional measures of sentiment for a given sentence.\n",
    "\n",
    "sentiment = []\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for tweet in em.Tweet:\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    sentiment.append(vs[\"compound\"])\n",
    "    \n",
    "em['Sentiment'] = sentiment\n",
    "print(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "2020-08-03   96.613335  100.653999   96.292000   99.000000   99.000000   \n",
      "2020-08-04   99.667336  101.827332   97.466667   99.133331   99.133331   \n",
      "2020-08-05   99.532669   99.989334   97.887337   99.001335   99.001335   \n",
      "2020-08-06   99.388664  101.153999   98.484001   99.305336   99.305336   \n",
      "2020-08-07   99.969330   99.983330   94.334000   96.847336   96.847336   \n",
      "2020-08-10   96.533333   97.166664   92.389336   94.571335   94.571335   \n",
      "2020-08-11   93.066666   94.666664   91.000000   91.625999   91.625999   \n",
      "2020-08-12   98.000000  105.666664   95.666664  103.650665  103.650665   \n",
      "2020-08-13  107.400002  110.078667  104.484001  108.066666  108.066666   \n",
      "2020-08-14  110.999336  111.253334  108.442665  110.047333  110.047333   \n",
      "2020-08-17  111.800003  123.057335  111.522003  122.375999  122.375999   \n",
      "2020-08-18  126.599335  128.259995  123.007332  125.806000  125.806000   \n",
      "2020-08-19  124.333336  127.400002  122.747330  125.235336  125.235336   \n",
      "2020-08-20  124.045334  134.799332  123.804001  133.455338  133.455338   \n",
      "2020-08-21  136.317337  139.699326  135.003326  136.665329  136.665329   \n",
      "2020-08-24  141.751999  141.933334  128.501328  134.279999  134.279999   \n",
      "2020-08-25  131.659332  135.196671  131.199997  134.889328  134.889328   \n",
      "2020-08-26  137.333328  144.399994  136.908661  143.544662  143.544662   \n",
      "2020-08-27  145.363998  153.039993  142.833328  149.250000  149.250000   \n",
      "2020-08-28  153.007996  154.565994  145.768005  147.559998  147.559998   \n",
      "2020-08-31  148.203339  166.713333  146.703339  166.106674  166.106674   \n",
      "2020-09-01  167.380005  167.496674  156.836670  158.350006  158.350006   \n",
      "2020-09-02  159.663330  159.679993  135.039993  149.123337  149.123337   \n",
      "2020-09-03  135.743332  143.933334  134.000000  135.666672  135.666672   \n",
      "2020-09-04  134.270004  142.666672  124.006668  139.440002  139.440002   \n",
      "2020-09-08  118.666664  122.913330  109.959999  110.070000  110.070000   \n",
      "2020-09-09  118.866669  123.000000  113.836670  122.093330  122.093330   \n",
      "2020-09-10  128.736664  132.996674  120.186668  123.779999  123.779999   \n",
      "2020-09-11  127.313332  127.500000  120.166664  124.239998  124.239998   \n",
      "2020-09-14  126.983330  140.000000  124.433334  139.873337  139.873337   \n",
      "2020-09-15  145.520004  153.979996  143.566666  149.919998  149.919998   \n",
      "2020-09-16  146.623337  152.596664  145.103333  147.253326  147.253326   \n",
      "2020-09-17  138.533340  145.929993  136.000000  141.143326  141.143326   \n",
      "2020-09-18  149.313339  150.333328  142.933334  147.383331  147.383331   \n",
      "2020-09-21  151.043335  151.893326  135.690002  149.796661  149.796661   \n",
      "2020-09-22  143.199997  145.919998  139.199997  141.410004  141.410004   \n",
      "2020-09-23  135.053329  137.383331  125.293335  126.786667  126.786667   \n",
      "2020-09-24  121.266670  133.166672  117.099998  129.263336  129.263336   \n",
      "2020-09-25  131.156662  136.243332  130.433334  135.779999  135.779999   \n",
      "2020-09-28  141.539993  142.693329  138.516663  140.399994  140.399994   \n",
      "2020-09-29  138.666672  142.833328  137.199997  139.690002  139.690002   \n",
      "\n",
      "               Volume   log_ret  sentiment 1  sentiment 2  sentiment 3  \\\n",
      "Date                                                                     \n",
      "2020-08-03  132139500  0.001346    -0.063480     0.180600     0.256986   \n",
      "2020-08-04  126225000 -0.001332     0.210500     0.000000     0.140500   \n",
      "2020-08-05   74217000  0.003066     0.000000     0.000000     0.000000   \n",
      "2020-08-06   89884500 -0.025063     0.000000     0.000000     0.312450   \n",
      "2020-08-07  133446000 -0.023781     0.004700    -0.170000     0.148000   \n",
      "2020-08-10  112834500 -0.031639     0.000000     0.255000     0.000000   \n",
      "2020-08-11  129387000  0.123311    -0.148000    -0.296000     0.000000   \n",
      "2020-08-12  327441000  0.041722     0.000000     0.000000    -0.205700   \n",
      "2020-08-13  306379500  0.018162     0.000000    -0.149600     0.023567   \n",
      "2020-08-14  188664000  0.106188     0.304225     0.034100     0.678833   \n",
      "2020-08-17  303634500  0.027643     0.562150     0.018120     0.121360   \n",
      "2020-08-18  247117500 -0.004546     0.363020     0.133967     0.289900   \n",
      "2020-08-19  183079500  0.063572     0.305000     0.422733    -0.047633   \n",
      "2020-08-20  309177000  0.023768    -0.019567     0.516500     0.167275   \n",
      "2020-08-21  322344000 -0.017608     0.000000     0.000000     0.000000   \n",
      "2020-08-24  300954000  0.004527    -0.098667     0.157650     0.159100   \n",
      "2020-08-25  159883500  0.062192     0.038600     0.421500     0.000000   \n",
      "2020-08-26  213591000  0.038977     0.000000     0.000000     0.388850   \n",
      "2020-08-27  355395000 -0.011388    -0.165775    -0.025800     0.000000   \n",
      "2020-08-28  301218000  0.118395     0.407200    -0.088520     0.210750   \n",
      "2020-08-31  355123200 -0.047822     0.016212     0.288983     0.142571   \n",
      "2020-09-01  269523300 -0.060034     0.000000    -0.401900     0.296000   \n",
      "2020-09-02  288528300 -0.094573     0.000000     0.000000     0.000000   \n",
      "2020-09-03  262788300  0.027433     0.935900     0.000000     0.000000   \n",
      "2020-09-04  330965700 -0.236518     0.041400     0.458800     0.000000   \n",
      "2020-09-08  346397100  0.103669     0.413550     0.229400    -0.092750   \n",
      "2020-09-09  238397400  0.013720     0.000000     0.358900     0.140800   \n",
      "2020-09-10  254791800  0.003709     0.000000     0.229400     0.000000   \n",
      "2020-09-11  182152500  0.118522     0.632800     0.000000     0.598300   \n",
      "2020-09-14  249061800  0.069365     0.357300     0.156960     0.274557   \n",
      "2020-09-15  291894600 -0.017947     0.361200     0.360050     0.381000   \n",
      "2020-09-16  216837900 -0.042379    -0.055167     0.638500     0.658800   \n",
      "2020-09-17  230337600  0.043261     0.510600     0.000000    -0.457600   \n",
      "2020-09-18  259220400  0.016242     0.000000     0.302950     0.636900   \n",
      "2020-09-21  328430400 -0.057615     0.251171     0.378343     0.296000   \n",
      "2020-09-22  238742400 -0.109158     0.000000     0.000000     0.285300   \n",
      "2020-09-23  285222600  0.019346     0.000000     0.000000     0.432567   \n",
      "2020-09-24  289683300  0.049184     0.000000     0.000000    -0.361200   \n",
      "2020-09-25  201625500  0.033460     0.390433     0.264600     0.187840   \n",
      "2020-09-28  149158800 -0.005070     0.107400    -0.113333     0.297491   \n",
      "2020-09-29  150657900  0.023442     0.000000     0.000000     0.000000   \n",
      "\n",
      "            sentiment 4  sentiment 5  \n",
      "Date                                  \n",
      "2020-08-03    -0.035900     0.167200  \n",
      "2020-08-04     0.000000     0.210750  \n",
      "2020-08-05     0.000000     0.000000  \n",
      "2020-08-06     0.000000     0.000000  \n",
      "2020-08-07     0.440400     0.296000  \n",
      "2020-08-10     0.200950     0.241227  \n",
      "2020-08-11     0.000000     0.408600  \n",
      "2020-08-12     0.000000     0.000000  \n",
      "2020-08-13     0.483650     0.488450  \n",
      "2020-08-14     0.664900     0.068560  \n",
      "2020-08-17     0.293742     0.179614  \n",
      "2020-08-18     0.273175     0.000000  \n",
      "2020-08-19     0.091125     0.902200  \n",
      "2020-08-20    -0.089100     0.313260  \n",
      "2020-08-21     0.000000     0.000000  \n",
      "2020-08-24     0.000000     0.306200  \n",
      "2020-08-25    -0.136600    -0.109400  \n",
      "2020-08-26     0.000000     0.222580  \n",
      "2020-08-27    -0.019933     0.526700  \n",
      "2020-08-28     0.167625     0.440400  \n",
      "2020-08-31     0.136008     0.221900  \n",
      "2020-09-01     0.000000     0.000000  \n",
      "2020-09-02     0.743000     0.000000  \n",
      "2020-09-03     0.750600     0.000000  \n",
      "2020-09-04     0.401900    -0.148000  \n",
      "2020-09-08     0.000000    -0.148000  \n",
      "2020-09-09     0.398200     0.000000  \n",
      "2020-09-10     0.508150     0.140500  \n",
      "2020-09-11     0.000000     0.177900  \n",
      "2020-09-14     0.278088     0.000000  \n",
      "2020-09-15     0.709600     0.176160  \n",
      "2020-09-16     0.074000     0.000000  \n",
      "2020-09-17     0.842400     0.000000  \n",
      "2020-09-18     0.000000     0.000000  \n",
      "2020-09-21     0.467160     0.482350  \n",
      "2020-09-22     0.229400    -0.271700  \n",
      "2020-09-23     0.579550     0.096750  \n",
      "2020-09-24     0.361200     0.000000  \n",
      "2020-09-25    -0.190900     0.000000  \n",
      "2020-09-28     0.423500     0.392260  \n",
      "2020-09-29     0.000000     0.000000  \n"
     ]
    }
   ],
   "source": [
    "import yfinance\n",
    "from datetime import datetime\n",
    "\n",
    "#Load daily Tesla Adjusted Close data and compute log returns\n",
    "tesla = yfinance.download(\"TSLA\",datetime(2020,8,1),datetime(2020,10,1)) \n",
    "tesla['log_ret'] = np.log(tesla['Adj Close'].shift(-1)) - np.log(tesla['Adj Close'])\n",
    "\n",
    "#Construct average sentiment on each day for each topic\n",
    "for k in range(K):\n",
    "    prev_d = datetime(2020,1,1).date()\n",
    "    avg_sentiment = []\n",
    "    for date in tesla.index.values:\n",
    "        d = pd.to_datetime(date).date()\n",
    "        avg = np.mean(em.loc[(em.Date <= d) & (em.Date > prev_d) & (em.Cluster == k)].Sentiment)\n",
    "        if np.isnan(avg):\n",
    "            avg_sentiment.append(0)\n",
    "        else:\n",
    "            avg_sentiment.append(avg)\n",
    "        prev_d = d\n",
    "    tesla['sentiment '+str(k+1)] = avg_sentiment\n",
    "\n",
    "#Drop final row\n",
    "tesla = tesla.iloc[:-1,]\n",
    "print(tesla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with Topic 1: 0.34416126831827\n",
      "Correlation with Topic 2: -0.2078878186430723\n",
      "Correlation with Topic 3: 0.00023618005818470378\n",
      "Correlation with Topic 4: -0.17769576983810267\n",
      "Correlation with Topic 5: 0.24683650182448783\n"
     ]
    }
   ],
   "source": [
    "#Find correlation between Tesla Returns and Elon Musk's Twitter account\n",
    "#Recall that we had low correlation when looking at all tweets\n",
    "for k in range(K):\n",
    "    print('Correlation with Topic '+str(k+1)+': '+str(tesla['log_ret'].corr(tesla['sentiment '+str(k+1)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# healthcare etf and 10 healthcare stock tickers\n",
    "tickers = [\n",
    "    'XLV',\n",
    "    'JNJ',\n",
    "    'PFE',\n",
    "    'MRK',\n",
    "    'UNH',\n",
    "    'ABT',\n",
    "    'ABBV',\n",
    "    'AMGN',\n",
    "    'MDT',\n",
    "    'DHR',\n",
    "    'CVS',\n",
    "    'MMM',\n",
    "    'GE',\n",
    "    'BA',\n",
    "    'CAT'\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
