{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accessible-broadway",
   "metadata": {},
   "source": [
    "# FA692 Homework 5\n",
    "# Due: Wednesday, April 19 @ 11:59PM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aware-spouse",
   "metadata": {},
   "source": [
    "Name: Ryan Shea\n",
    "\n",
    "Date: 2023-04-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fundamental-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed of random number generator\n",
    "CWID = 10445281 #Place here your Campus wide ID number, this will personalize\n",
    "#your results, but still maintain the reproduceable nature of using seeds.\n",
    "#If you ever need to reset the seed in this assignment, use this as your seed\n",
    "#Papers that use -1 as this CWID variable will earn 0's so make sure you change\n",
    "#this value before you submit your work.\n",
    "personal = CWID % 10000\n",
    "np.random.seed(personal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-rebound",
   "metadata": {},
   "source": [
    "## Question 1 (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-shoulder",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "Use the `yfinance` package (or other method of your choice) to obtain the daily adjusted close prices for the S&P500 (`SPY`) from January 1, 2023 to March 15, 2023.  You should inspect the dates for your data to make sure you are including everything appropriately.  Create a data frame (or array) of the daily log returns of this stock; you may concatenate this to your price data.  Use the `print` command to display your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ae435c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Date\n",
      "2023-01-04    0.007691\n",
      "2023-01-05   -0.011479\n",
      "2023-01-06    0.022673\n",
      "2023-01-09   -0.000567\n",
      "2023-01-10    0.006988\n",
      "Name: Adj Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "data = yf.download('SPY', start='2023-01-01', end='2023-03-15')\n",
    "ret = data['Adj Close'].apply(np.log).diff().dropna()\n",
    "\n",
    "print(ret.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-range",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "Scrape data from the Bloomberg `@business` Twitter account from January 1, 2023 to March 15, 2023. Save this data to a Data Frame with time stamps. Additionally, save all the collected data to a text file with time stamps. You will need to submit the text file along with your work (-5 points if not submitted).\n",
    "\n",
    "Note: Bloomberg tweets sometimes include the pipe \"|\". I recomment using tilde \"~\" as a delimiter instead.\n",
    "\n",
    "Hint: Because saving the tweets can take a long time, you can comment that code out before exporting to pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "invisible-understanding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-14 19:40:29-04:00</td>\n",
       "      <td>One Japanese fintech firm is making it compuls...</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-14 19:40:29-04:00</td>\n",
       "      <td>An unlikely startup guru has emerged in Japan,...</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-14 19:35:41-04:00</td>\n",
       "      <td>Some US cities are late in making financial di...</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-14 19:31:07-04:00</td>\n",
       "      <td>The shipping industry is looking to rethink ev...</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-14 19:25:09-04:00</td>\n",
       "      <td>A biotech wants to cut fashion waste by using ...</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26809</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>Toymakers have found a new group of customers:...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26810</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>Belarusian hackers and dissidents determined t...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26811</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>Landlords are taking out millions in loans to ...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26812</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>It took a pandemic to make a dent in US inequa...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26813</th>\n",
       "      <td>2022-12-31 19:00:08-05:00</td>\n",
       "      <td>A planned train line in Mexico is billions ove...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26814 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Time  \\\n",
       "0     2023-03-14 19:40:29-04:00   \n",
       "1     2023-03-14 19:40:29-04:00   \n",
       "2     2023-03-14 19:35:41-04:00   \n",
       "3     2023-03-14 19:31:07-04:00   \n",
       "4     2023-03-14 19:25:09-04:00   \n",
       "...                         ...   \n",
       "26809 2022-12-31 19:00:09-05:00   \n",
       "26810 2022-12-31 19:00:09-05:00   \n",
       "26811 2022-12-31 19:00:09-05:00   \n",
       "26812 2022-12-31 19:00:09-05:00   \n",
       "26813 2022-12-31 19:00:08-05:00   \n",
       "\n",
       "                                                   Tweet        Date  \n",
       "0      One Japanese fintech firm is making it compuls...  2023-03-14  \n",
       "1      An unlikely startup guru has emerged in Japan,...  2023-03-14  \n",
       "2      Some US cities are late in making financial di...  2023-03-14  \n",
       "3      The shipping industry is looking to rethink ev...  2023-03-14  \n",
       "4      A biotech wants to cut fashion waste by using ...  2023-03-14  \n",
       "...                                                  ...         ...  \n",
       "26809  Toymakers have found a new group of customers:...  2022-12-31  \n",
       "26810  Belarusian hackers and dissidents determined t...  2022-12-31  \n",
       "26811  Landlords are taking out millions in loans to ...  2022-12-31  \n",
       "26812  It took a pandemic to make a dent in US inequa...  2022-12-31  \n",
       "26813  A planned train line in Mexico is billions ove...  2022-12-31  \n",
       "\n",
       "[26814 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import snscrape.modules.twitter as tw\n",
    "\n",
    "# f = open('business.txt', 'w', encoding='utf-8')\n",
    "\n",
    "# for tweet in tw.TwitterSearchScraper(query=\"(from:business) since:2023-01-01 until:2023-03-15\").get_items():\n",
    "#     date_str = tweet.date.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "#     date_str = date_str[:-2] + \":\" + date_str[-2:]\n",
    "#     #f.write(date_str + \"|\" + tweet.content + \"\\n\")\n",
    "#     f.write(date_str + \"~\" + tweet.rawContent + \"\\n\")\n",
    "# f.close()\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import pytz\n",
    "\n",
    "business = []\n",
    "dates = []\n",
    "f = open('business.txt', 'r', encoding='utf-8')\n",
    "\n",
    "for l in f:\n",
    "    line = l.split('~')\n",
    "    date_str = line[0]\n",
    "    try:\n",
    "        date_time = dt.fromisoformat(date_str)\n",
    "        date_time = date_time.astimezone(pytz.timezone(\"US/Eastern\"))\n",
    "        line[0] = date_time\n",
    "        line[1] = line[1][:-1]\n",
    "        business.append(line)\n",
    "        dates.append(date_time.date())\n",
    "    except:\n",
    "        business[-1][1] += \" \"+l[:-1]\n",
    "f.close()\n",
    "\n",
    "business = pd.DataFrame(business, columns=['Time', 'Tweet'])\n",
    "business['Date'] = dates\n",
    "\n",
    "business"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea702e8",
   "metadata": {},
   "source": [
    "## Question 2 (30pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd17f6",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Use Latent Dirichlet Allocation to cluster the tweets into at least 8 topics. You may use your favorite method for tokenizing and vectorizing.\n",
    "Display the top 10 words for each of your topic.\n",
    "Add a column to your Data Frame of Tweets to label the topic associated with each Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d381de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process for Latent Dirichlet Allocation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Initialize regex tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # Tokenizes by word\n",
    "\n",
    "# Term frequency-inverse document frequency\n",
    "tfidf = TfidfVectorizer(lowercase=True,stop_words='english',ngram_range=(1,1),tokenizer=tokenizer.tokenize)\n",
    "tfidf_data = tfidf.fit_transform(business['Tweet'].tolist())\n",
    "tfidf_feature_names = tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2559d27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  ['t', 'https', 's', 'ukraine', 'president', 'biden', 'latest', 'russia', 'says', 'updates']\n",
      "Topic 2:  ['t', 'https', 's', 'year', 'musk', 'new', 'elon', 'says', 'inflation', 'market']\n",
      "Topic 3:  ['t', 'https', 's', 'new', 'opinion', 'billion', 'world', 'big', 'climate', 'people']\n",
      "Topic 4:  ['t', 'https', 's', 'new', 'year', 'says', 'bank', 'crypto', 'world', 'billion']\n",
      "Topic 5:  ['t', 'https', 's', 'new', 'opinion', 'year', 'says', 'york', 'world', 'london']\n",
      "Topic 6:  ['t', 'https', 's', 'new', 'year', 'best', 'climate', 'tv', 'million', 'billion']\n",
      "Topic 7:  ['s', 't', 'https', 'year', 'adani', 'new', 'china', 'market', 'says', 'know']\n",
      "Topic 8:  ['t', 'https', 's', 'bank', 'rate', 'inflation', 'fed', 'says', 'new', 'year']\n",
      "Topic 9:  ['t', 'https', 's', 'china', 'says', 'world', 'secretary', 'bank', 'week', 'year']\n",
      "Topic 10:  ['t', 'https', 's', 'new', 'china', 'year', 'uk', 'says', 'people', 'finance']\n"
     ]
    }
   ],
   "source": [
    "# Run Latent Dirichlet Allocation\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "K = 10 # Number of topics\n",
    "\n",
    "# Try with term frequency-inverse document frequency:\n",
    "LDA_TFIDF = LatentDirichletAllocation(n_components=K)\n",
    "LDA_TFIDF_Matrix = LDA_TFIDF.fit_transform(tfidf_data) # Fits and transforms the dataset\n",
    "LDA_TFIDF_Components = LDA_TFIDF.components_ # Get the components\n",
    "\n",
    "def display_topics(components, feature_names, no_top_words):\n",
    "    for index, topic in enumerate(components):\n",
    "        top_terms_list = [feature_names[i] for i in topic.argsort()[:-no_top_words-1:-1]]\n",
    "        print(\"Topic \"+str(index+1)+\": \",top_terms_list)\n",
    "\n",
    "no_top_words = 10\n",
    "\n",
    "display_topics(LDA_TFIDF_Components, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0965e6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-14 19:40:29-04:00</td>\n",
       "      <td>One Japanese fintech firm is making it compuls...</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-14 19:40:29-04:00</td>\n",
       "      <td>An unlikely startup guru has emerged in Japan,...</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-14 19:35:41-04:00</td>\n",
       "      <td>Some US cities are late in making financial di...</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-14 19:31:07-04:00</td>\n",
       "      <td>The shipping industry is looking to rethink ev...</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-14 19:25:09-04:00</td>\n",
       "      <td>A biotech wants to cut fashion waste by using ...</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26809</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>Toymakers have found a new group of customers:...</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26810</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>Belarusian hackers and dissidents determined t...</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26811</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>Landlords are taking out millions in loans to ...</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26812</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>It took a pandemic to make a dent in US inequa...</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26813</th>\n",
       "      <td>2022-12-31 19:00:08-05:00</td>\n",
       "      <td>A planned train line in Mexico is billions ove...</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26814 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Time  \\\n",
       "0     2023-03-14 19:40:29-04:00   \n",
       "1     2023-03-14 19:40:29-04:00   \n",
       "2     2023-03-14 19:35:41-04:00   \n",
       "3     2023-03-14 19:31:07-04:00   \n",
       "4     2023-03-14 19:25:09-04:00   \n",
       "...                         ...   \n",
       "26809 2022-12-31 19:00:09-05:00   \n",
       "26810 2022-12-31 19:00:09-05:00   \n",
       "26811 2022-12-31 19:00:09-05:00   \n",
       "26812 2022-12-31 19:00:09-05:00   \n",
       "26813 2022-12-31 19:00:08-05:00   \n",
       "\n",
       "                                                   Tweet        Date  Cluster  \n",
       "0      One Japanese fintech firm is making it compuls...  2023-03-14        9  \n",
       "1      An unlikely startup guru has emerged in Japan,...  2023-03-14        7  \n",
       "2      Some US cities are late in making financial di...  2023-03-14        6  \n",
       "3      The shipping industry is looking to rethink ev...  2023-03-14        6  \n",
       "4      A biotech wants to cut fashion waste by using ...  2023-03-14        2  \n",
       "...                                                  ...         ...      ...  \n",
       "26809  Toymakers have found a new group of customers:...  2022-12-31        2  \n",
       "26810  Belarusian hackers and dissidents determined t...  2022-12-31        3  \n",
       "26811  Landlords are taking out millions in loans to ...  2022-12-31        5  \n",
       "26812  It took a pandemic to make a dent in US inequa...  2022-12-31        9  \n",
       "26813  A planned train line in Mexico is billions ove...  2022-12-31        0  \n",
       "\n",
       "[26814 rows x 4 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.argmax(LDA_TFIDF_Matrix, axis=1)\n",
    "business['Cluster'] = labels\n",
    "business"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054e182",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using your favorite sentiment analyzer (e.g., `vaderSentiment`), find the average sentiment for each topic from your topic modeling for the headlines on each date that data was collected. Concatenate this sentiment score to your data frame of log returns. Use the `print` command to display your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e5e3656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 log      sent\n",
      "Date                          \n",
      "2023-01-04  0.007691 -0.010883\n",
      "2023-01-05 -0.011479 -0.032084\n",
      "2023-01-06  0.022673 -0.001990\n",
      "2023-01-09 -0.000567  0.048762\n",
      "2023-01-10  0.006988 -0.051226\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentiment = []\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for tweet in business['Tweet']:\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    sentiment.append(vs['compound'])\n",
    "\n",
    "business['Sentiment'] = sentiment\n",
    "\n",
    "df = pd.DataFrame({'log': ret})\n",
    "df['sent'] = business.pivot_table(index='Date', values='Sentiment', aggfunc='mean')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-tiger",
   "metadata": {},
   "source": [
    "## Question 3 (20pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-lodging",
   "metadata": {},
   "source": [
    "### Question 3.1\n",
    "Linearly regress `SPY` returns as a function of the lagged returns (2 lags).\n",
    "This should be of the form $r_{t} = \\beta_0 + \\beta_{1} r_{t-1} + \\beta_{2} r_{t-2}$.\n",
    "Evaluate the performance of this model with the mean squared error of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "45e768f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (46, 36)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "      <th>sent</th>\n",
       "      <th>log_1</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>log_2</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>cluster_0</th>\n",
       "      <th>cluster_0_1</th>\n",
       "      <th>cluster_0_2</th>\n",
       "      <th>cluster_1</th>\n",
       "      <th>...</th>\n",
       "      <th>cluster_6_2</th>\n",
       "      <th>cluster_7</th>\n",
       "      <th>cluster_7_1</th>\n",
       "      <th>cluster_7_2</th>\n",
       "      <th>cluster_8</th>\n",
       "      <th>cluster_8_1</th>\n",
       "      <th>cluster_8_2</th>\n",
       "      <th>cluster_9</th>\n",
       "      <th>cluster_9_1</th>\n",
       "      <th>cluster_9_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-06</th>\n",
       "      <td>0.022673</td>\n",
       "      <td>-0.001990</td>\n",
       "      <td>-0.011479</td>\n",
       "      <td>-0.032084</td>\n",
       "      <td>0.007691</td>\n",
       "      <td>-0.010883</td>\n",
       "      <td>-0.023586</td>\n",
       "      <td>-0.019584</td>\n",
       "      <td>-0.003480</td>\n",
       "      <td>-0.010891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095122</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>-0.119121</td>\n",
       "      <td>-0.018074</td>\n",
       "      <td>-0.023271</td>\n",
       "      <td>-0.087319</td>\n",
       "      <td>0.058240</td>\n",
       "      <td>-0.017962</td>\n",
       "      <td>-0.037485</td>\n",
       "      <td>-0.062720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-09</th>\n",
       "      <td>-0.000567</td>\n",
       "      <td>0.048762</td>\n",
       "      <td>0.022673</td>\n",
       "      <td>-0.001990</td>\n",
       "      <td>-0.011479</td>\n",
       "      <td>-0.032084</td>\n",
       "      <td>0.017994</td>\n",
       "      <td>-0.023586</td>\n",
       "      <td>-0.019584</td>\n",
       "      <td>0.031297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040583</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>-0.119121</td>\n",
       "      <td>0.067354</td>\n",
       "      <td>-0.023271</td>\n",
       "      <td>-0.087319</td>\n",
       "      <td>0.111824</td>\n",
       "      <td>-0.017962</td>\n",
       "      <td>-0.037485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-10</th>\n",
       "      <td>0.006988</td>\n",
       "      <td>-0.051226</td>\n",
       "      <td>-0.000567</td>\n",
       "      <td>0.048762</td>\n",
       "      <td>0.022673</td>\n",
       "      <td>-0.001990</td>\n",
       "      <td>-0.098214</td>\n",
       "      <td>0.017994</td>\n",
       "      <td>-0.023586</td>\n",
       "      <td>0.123016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002362</td>\n",
       "      <td>-0.005892</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.067354</td>\n",
       "      <td>-0.023271</td>\n",
       "      <td>0.037836</td>\n",
       "      <td>0.111824</td>\n",
       "      <td>-0.017962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-11</th>\n",
       "      <td>0.012569</td>\n",
       "      <td>-0.006794</td>\n",
       "      <td>0.006988</td>\n",
       "      <td>-0.051226</td>\n",
       "      <td>-0.000567</td>\n",
       "      <td>0.048762</td>\n",
       "      <td>-0.041914</td>\n",
       "      <td>-0.098214</td>\n",
       "      <td>0.017994</td>\n",
       "      <td>-0.004346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081896</td>\n",
       "      <td>-0.030783</td>\n",
       "      <td>-0.005892</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.026706</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.067354</td>\n",
       "      <td>-0.010175</td>\n",
       "      <td>0.037836</td>\n",
       "      <td>0.111824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-12</th>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>0.012569</td>\n",
       "      <td>-0.006794</td>\n",
       "      <td>0.006988</td>\n",
       "      <td>-0.051226</td>\n",
       "      <td>0.055011</td>\n",
       "      <td>-0.041914</td>\n",
       "      <td>-0.098214</td>\n",
       "      <td>-0.059940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133607</td>\n",
       "      <td>0.066988</td>\n",
       "      <td>-0.030783</td>\n",
       "      <td>-0.005892</td>\n",
       "      <td>-0.063157</td>\n",
       "      <td>0.026706</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.091786</td>\n",
       "      <td>-0.010175</td>\n",
       "      <td>0.037836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 log      sent     log_1    sent_1     log_2    sent_2  \\\n",
       "Date                                                                     \n",
       "2023-01-06  0.022673 -0.001990 -0.011479 -0.032084  0.007691 -0.010883   \n",
       "2023-01-09 -0.000567  0.048762  0.022673 -0.001990 -0.011479 -0.032084   \n",
       "2023-01-10  0.006988 -0.051226 -0.000567  0.048762  0.022673 -0.001990   \n",
       "2023-01-11  0.012569 -0.006794  0.006988 -0.051226 -0.000567  0.048762   \n",
       "2023-01-12  0.003634  0.012535  0.012569 -0.006794  0.006988 -0.051226   \n",
       "\n",
       "            cluster_0  cluster_0_1  cluster_0_2  cluster_1  ...  cluster_6_2  \\\n",
       "Date                                                        ...                \n",
       "2023-01-06  -0.023586    -0.019584    -0.003480  -0.010891  ...     0.095122   \n",
       "2023-01-09   0.017994    -0.023586    -0.019584   0.031297  ...     0.040583   \n",
       "2023-01-10  -0.098214     0.017994    -0.023586   0.123016  ...    -0.002362   \n",
       "2023-01-11  -0.041914    -0.098214     0.017994  -0.004346  ...     0.081896   \n",
       "2023-01-12   0.055011    -0.041914    -0.098214  -0.059940  ...    -0.133607   \n",
       "\n",
       "            cluster_7  cluster_7_1  cluster_7_2  cluster_8  cluster_8_1  \\\n",
       "Date                                                                      \n",
       "2023-01-06   0.009463    -0.119121    -0.018074  -0.023271    -0.087319   \n",
       "2023-01-09   0.078900     0.009463    -0.119121   0.067354    -0.023271   \n",
       "2023-01-10  -0.005892     0.078900     0.009463   0.000242     0.067354   \n",
       "2023-01-11  -0.030783    -0.005892     0.078900   0.026706     0.000242   \n",
       "2023-01-12   0.066988    -0.030783    -0.005892  -0.063157     0.026706   \n",
       "\n",
       "            cluster_8_2  cluster_9  cluster_9_1  cluster_9_2  \n",
       "Date                                                          \n",
       "2023-01-06     0.058240  -0.017962    -0.037485    -0.062720  \n",
       "2023-01-09    -0.087319   0.111824    -0.017962    -0.037485  \n",
       "2023-01-10    -0.023271   0.037836     0.111824    -0.017962  \n",
       "2023-01-11     0.067354  -0.010175     0.037836     0.111824  \n",
       "2023-01-12     0.000242   0.091786    -0.010175     0.037836  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    df[f'log_{i}'] = df['log'].shift(i)\n",
    "    df[f'sent_{i}'] = df['sent'].shift(i)\n",
    "\n",
    "\n",
    "# average sentiment on each day for each topic\n",
    "for k in range(K):\n",
    "    df[f'cluster_{k}'] = business[business['Cluster'] == k] \\\n",
    "        .pivot_table(index='Date', values='Sentiment', aggfunc='mean')\n",
    "    # lagged sentiment for each topic\n",
    "    for i in range(1, 3):\n",
    "        df[f'cluster_{k}_{i}'] = df[f'cluster_{k}'].shift(i)\n",
    "        \n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"{df.shape = }\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ea39bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Intercept: 0.0007263457920817806\n",
      "\n",
      "        log_1: 0.05614851635772608\n",
      "        log_2: -0.12133150745716237\n",
      "\n",
      "MSE = 0.00011083584269359552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def linreg(X):\n",
    "    y = df['log']\n",
    "\n",
    "    lr = LinearRegression().fit(X, y)\n",
    "    y_pred = lr.predict(X)\n",
    "\n",
    "    print(f\"    Intercept: {lr.intercept_}\", end='\\n\\n')\n",
    "\n",
    "    for feat, coef in zip(X.columns, lr.coef_):\n",
    "        print(f\"{feat:>13}: {coef}\")\n",
    "    \n",
    "    MSE = mean_squared_error(y, y_pred)\n",
    "    print()\n",
    "    print(f\"{MSE = }\")\n",
    "\n",
    "    return lr, MSE\n",
    "\n",
    "X = df[['log_1', 'log_2']]\n",
    "\n",
    "lr1, mse1 = linreg(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4317f6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['log', 'sent', 'log_1', 'sent_1', 'log_2', 'sent_2', 'cluster_0',\n",
       "       'cluster_0_1', 'cluster_0_2', 'cluster_1', 'cluster_1_1', 'cluster_1_2',\n",
       "       'cluster_2', 'cluster_2_1', 'cluster_2_2', 'cluster_3', 'cluster_3_1',\n",
       "       'cluster_3_2', 'cluster_4', 'cluster_4_1', 'cluster_4_2', 'cluster_5',\n",
       "       'cluster_5_1', 'cluster_5_2', 'cluster_6', 'cluster_6_1', 'cluster_6_2',\n",
       "       'cluster_7', 'cluster_7_1', 'cluster_7_2', 'cluster_8', 'cluster_8_1',\n",
       "       'cluster_8_2', 'cluster_9', 'cluster_9_1', 'cluster_9_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7b302fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log_1',\n",
       " 'sent_1',\n",
       " 'log_2',\n",
       " 'sent_2',\n",
       " 'cluster_0_1',\n",
       " 'cluster_0_2',\n",
       " 'cluster_1_1',\n",
       " 'cluster_1_2',\n",
       " 'cluster_2_1',\n",
       " 'cluster_2_2',\n",
       " 'cluster_3_1',\n",
       " 'cluster_3_2',\n",
       " 'cluster_4_1',\n",
       " 'cluster_4_2',\n",
       " 'cluster_5_1',\n",
       " 'cluster_5_2',\n",
       " 'cluster_6_1',\n",
       " 'cluster_6_2',\n",
       " 'cluster_7_1',\n",
       " 'cluster_7_2',\n",
       " 'cluster_8_1',\n",
       " 'cluster_8_2',\n",
       " 'cluster_9_1',\n",
       " 'cluster_9_2']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = []  # all lagged variables\n",
    "for c in df.columns:\n",
    "    if c.endswith('_1') or c.endswith('_2'):\n",
    "        if c not in ['cluster_1', 'cluster_2']:\n",
    "            X_all.append(c)\n",
    "X_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1513f2",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "Linearly regress `SPY` returns as a function of the lagged sentiment (2 lags) for each topic.\n",
    "This should be of the form $r_{t} = \\beta_0 + \\sum_{k = 1}^K (\\beta_{k,1} s_{k,t-1} + \\beta_{k,2} s_{k,t-2})$ with $K$ topics total.\n",
    "Evaluate the performance of this model with the mean squared error of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2e2c648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Intercept: 0.0034099293862093457\n",
      "\n",
      "  cluster_0_1: -0.04368060661279426\n",
      "  cluster_0_2: 0.005306805024557843\n",
      "  cluster_1_1: -0.02532314214301729\n",
      "  cluster_1_2: 0.0035390821675976313\n",
      "  cluster_2_1: -0.004618287090389198\n",
      "  cluster_2_2: 0.0013070076463209182\n",
      "  cluster_3_1: -0.03397836845188566\n",
      "  cluster_3_2: 0.019219671174599932\n",
      "  cluster_4_1: 0.00012094105634543628\n",
      "  cluster_4_2: -0.017480047476365494\n",
      "  cluster_5_1: 0.006062232151662178\n",
      "  cluster_5_2: -0.006305998425245751\n",
      "  cluster_6_1: 0.012319830485283376\n",
      "  cluster_6_2: -0.008357687897851367\n",
      "  cluster_7_1: -0.05826020970623672\n",
      "  cluster_7_2: 0.015010251067016667\n",
      "  cluster_8_1: 0.014649675440873975\n",
      "  cluster_8_2: -0.015446873701099852\n",
      "  cluster_9_1: -0.024727419695585454\n",
      "  cluster_9_2: -0.06301417067434663\n",
      "\n",
      "MSE = 5.873463195465605e-05\n"
     ]
    }
   ],
   "source": [
    "cols = X_all[4:]\n",
    "\n",
    "X = df[cols]\n",
    "\n",
    "lr2, mse2 = linreg(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd5f26",
   "metadata": {},
   "source": [
    "### Question 3.3\n",
    "Linearly regress `SPY` returns as a function of the lagged returns and sentiment for each topic (2 lags each).\n",
    "This should be of the form $r_{t} = \\beta_0 + \\beta_{1,r} r_{t-1} + \\beta_{2,r} r_{t-2} + \\sum_{k = 1}^K (\\beta_{k,1,s} s_{k,t-1} + \\beta_{k,2,s} s_{k,t-2})$ with $K$ topics total. \n",
    "Evaluate the performance of this model with the mean squared error of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "81e2f9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Intercept: 0.003342826903150288\n",
      "\n",
      "        log_1: 0.03709925547610192\n",
      "        log_2: 0.19104101073241453\n",
      "  cluster_0_1: -0.05683016263096051\n",
      "  cluster_0_2: 0.008730169724015437\n",
      "  cluster_1_1: -0.033370820549430406\n",
      "  cluster_1_2: 0.006106174567108706\n",
      "  cluster_2_1: -0.004952829977398088\n",
      "  cluster_2_2: 0.008274109715000021\n",
      "  cluster_3_1: -0.03898957360633067\n",
      "  cluster_3_2: 0.026959929334309348\n",
      "  cluster_4_1: -0.004767614559797518\n",
      "  cluster_4_2: -0.01635396102549187\n",
      "  cluster_5_1: 0.012155045239428171\n",
      "  cluster_5_2: -0.0037324283474021633\n",
      "  cluster_6_1: 0.013797178927053393\n",
      "  cluster_6_2: -0.01316457097060699\n",
      "  cluster_7_1: -0.06806996279986878\n",
      "  cluster_7_2: 0.010119542283322337\n",
      "  cluster_8_1: 0.02218959374586673\n",
      "  cluster_8_2: -0.009212673528608889\n",
      "  cluster_9_1: -0.030146164476976458\n",
      "  cluster_9_2: -0.061379559771011984\n",
      "\n",
      "MSE = 5.65466064831002e-05\n"
     ]
    }
   ],
   "source": [
    "X = [x for x in X_all if x not in ['sent_1', 'sent_2']]\n",
    "\n",
    "lr3, mse3 = linreg(df[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3c1bdba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Intercept: 0.0030591796178134276\n",
      "\n",
      "        log_1: 0.02857120115675503\n",
      "       sent_1: 0.38674942714117283\n",
      "        log_2: 0.2216032936819905\n",
      "       sent_2: -0.1586331355061179\n",
      "  cluster_0_1: -0.12373452012238063\n",
      "  cluster_0_2: 0.036324554264927825\n",
      "  cluster_1_1: -0.06716935547702339\n",
      "  cluster_1_2: 0.021939955436803014\n",
      "  cluster_2_1: -0.0435552468336734\n",
      "  cluster_2_2: 0.026675652037434133\n",
      "  cluster_3_1: -0.07843244347502183\n",
      "  cluster_3_2: 0.044055687751873844\n",
      "  cluster_4_1: -0.047667537947838866\n",
      "  cluster_4_2: -0.0005847135591747391\n",
      "  cluster_5_1: -0.014909938819301015\n",
      "  cluster_5_2: 0.008807501525195167\n",
      "  cluster_6_1: -0.03529473055470836\n",
      "  cluster_6_2: 0.0017962565828821095\n",
      "  cluster_7_1: -0.1225198939148208\n",
      "  cluster_7_2: 0.03117785912239474\n",
      "  cluster_8_1: -0.0048327606983597825\n",
      "  cluster_8_2: -0.0030178099409436257\n",
      "  cluster_9_1: -0.05679599234913632\n",
      "  cluster_9_2: -0.04796822768550321\n",
      "\n",
      "MSE = 5.512289512560037e-05\n"
     ]
    }
   ],
   "source": [
    "# do one more linear regression with all the variables\n",
    "lr4, mse4 = linreg(df[X_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6f3b9df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** |FEATURE IMPORTANCES| ******\n",
      "        log_1 : 0.018223593654228005\n",
      "       sent_1 : 0.12483490714009944\n",
      "        log_2 : 0.023537824999403732\n",
      "       sent_2 : 0.025877496589269734\n",
      "  cluster_0_1 : 0.02268366002912877\n",
      "  cluster_0_2 : 0.03085370520019216\n",
      "  cluster_1_1 : 0.025111184342087464\n",
      "  cluster_1_2 : 0.021391833294298664\n",
      "  cluster_2_1 : 0.020159626316692773\n",
      "  cluster_2_2 : 0.026643605837924772\n",
      "  cluster_3_1 : 0.025638679491116464\n",
      "  cluster_3_2 : 0.02939705346056652\n",
      "  cluster_4_1 : 0.017520522307446662\n",
      "  cluster_4_2 : 0.06721384442730202\n",
      "  cluster_5_1 : 0.04137818135949007\n",
      "  cluster_5_2 : 0.05353019861670025\n",
      "  cluster_6_1 : 0.023037469144374788\n",
      "  cluster_6_2 : 0.011289276122054149\n",
      "  cluster_7_1 : 0.0522710532171207\n",
      "  cluster_7_2 : 0.015322696618652144\n",
      "  cluster_8_1 : 0.07552840851318657\n",
      "  cluster_8_2 : 0.046406809646948266\n",
      "  cluster_9_1 : 0.07275337569462464\n",
      "  cluster_9_2 : 0.12939499397709114\n",
      "\n",
      "      MSE: 1.767404789961455e-05\n"
     ]
    }
   ],
   "source": [
    "# see with random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(df[X_all], df['log'])\n",
    "\n",
    "preds = rf.predict(df[X_all])\n",
    "\n",
    "print(\"*\" * 6, \"|FEATURE IMPORTANCES|\", \"*\" * 6)\n",
    "for feat, imp in zip(df[X_all].columns, rf.feature_importances_):\n",
    "    print(f\"{feat:>13} : {imp}\")\n",
    "\n",
    "rf_mse = mean_squared_error(df['log'], preds)\n",
    "print()\n",
    "print(f\"      MSE: {rf_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c248fc",
   "metadata": {},
   "source": [
    "### Question 3.4\n",
    "Compare the performance of these 3 linear regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "163c0116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse1 = 0.00011083584269359552\n",
      "mse2 = 5.873463195465605e-05\n",
      "mse3 = 5.65466064831002e-05\n",
      "\n",
      "mse4 = 5.512289512560037e-05\n",
      "rf_mse = 1.767404789961455e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"{mse1 = }\")\n",
    "print(f\"{mse2 = }\")\n",
    "print(f\"{mse3 = }\")\n",
    "print()\n",
    "print(f\"{mse4 = }\")\n",
    "print(f\"{rf_mse = }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2f482be",
   "metadata": {},
   "source": [
    "You can see that the best performing model was the one with both the lagged returns as well as the lagged cluster sentiments as well. The worst was the one that just had lagged returns. The MSE can get even better if you take an average sentiment for each day on top of the clusters. Finally, the random forest does significantly better than all of the linear regression which suggests that a more complicated model will do better than the multiple linear regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b3544",
   "metadata": {},
   "source": [
    "## Question 4 (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1b237",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Compare the performance of the various regressions utilized to those in prior homeworks. Do you find that applying topic modeling is beneficial in your analysis? Explain why or why not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "623e5d38",
   "metadata": {},
   "source": [
    "Topic modeling is definitely useful in my analysis. Comparing the results to homework 4 (pos, neg, neutral semtiments) the error is decreased slighly when looking at the linear regression. It is not a massive difference but it becomes larger when I add the average sentiment for each day in as well. This error could be decreased even more if I were to add a pos, neg, neutral sentiment in as well as they tended to do better than the aggregated score. \n",
    "\n",
    "In conclusion, topic modeling is definitely beneficial in analysis but is one piece of the puzzle: when combining it with other parts as well it becomes even stronger and more accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
