{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accessible-broadway",
   "metadata": {},
   "source": [
    "# FA692 Homework 3\n",
    "# Due: Wednesday, April 5 @ 11:59PM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aware-spouse",
   "metadata": {},
   "source": [
    "Name: Ryan Shea\n",
    "\n",
    "Date: 2023-04-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fundamental-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed of random number generator\n",
    "CWID = 10445281 #Place here your Campus wide ID number, this will personalize\n",
    "#your results, but still maintain the reproduceable nature of using seeds.\n",
    "#If you ever need to reset the seed in this assignment, use this as your seed\n",
    "#Papers that use -1 as this CWID variable will earn 0's so make sure you change\n",
    "#this value before you submit your work.\n",
    "personal = CWID % 10000\n",
    "np.random.seed(personal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-rebound",
   "metadata": {},
   "source": [
    "## Question 1 (20pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-shoulder",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "Use the `yfinance` package (or other method of your choice) to obtain the daily adjusted close prices for the S&P500 (`SPY`) from January 1, 2023 to March 15, 2023.  You should inspect the dates for your data to make sure you are including everything appropriately.  Create a data frame (or array) of the daily log returns of this stock; you may concatenate this to your price data.  Use the `print` command to display your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae435c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "2023-01-03  384.369995  386.429993  377.829987  380.820007  379.372131   \n",
      "2023-01-04  383.179993  385.880005  380.000000  383.760010  382.300964   \n",
      "2023-01-05  381.720001  381.839996  378.760010  379.380005  377.937592   \n",
      "2023-01-06  382.609985  389.250000  379.410004  388.079987  386.604492   \n",
      "2023-01-09  390.369995  393.700012  387.670013  387.859985  386.385345   \n",
      "\n",
      "               Volume   log_ret  \n",
      "Date                             \n",
      "2023-01-03   74850700       NaN  \n",
      "2023-01-04   85934100  0.007691  \n",
      "2023-01-05   76970500 -0.011479  \n",
      "2023-01-06  104189600  0.022673  \n",
      "2023-01-09   73978100 -0.000567  \n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "data = yf.download('SPY', start='2023-01-01', end='2023-03-15')\n",
    "\n",
    "data['log_ret'] = np.log(data['Adj Close']/data['Adj Close'].shift(1))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-range",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "Scrape data from the Bloomberg `@business` Twitter account from January 1, 2023 to March 15, 2023. Save this data to a Data Frame with time stamps. Additionally, save all the collected data to a text file with time stamps. You will need to submit the text file along with your work (-5 points if not submitted).\n",
    "\n",
    "Note: Bloomberg tweets sometimes include the pipe \"|\". I recomment using tilde \"~\" as a delimiter instead.\n",
    "\n",
    "Hint: Because saving the tweets can take a long time, you can comment that code out before exporting to pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "invisible-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import snscrape.modules.twitter as tw\n",
    "\n",
    "# f = open('business3.txt', 'w', encoding='utf-8')\n",
    "\n",
    "# for tweet in tw.TwitterSearchScraper(query=\"(from:business) since:2023-01-01 until:2023-03-15\").get_items():\n",
    "#     date_str = tweet.date.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "#     date_str = date_str[:-2] + \":\" + date_str[-2:]\n",
    "#     #f.write(date_str + \"|\" + tweet.content + \"\\n\")\n",
    "#     f.write(date_str + \"~\" + tweet.rawContent + \"\\n\")\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd7dc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-14 19:40:29-04:00</td>\n",
       "      <td>One Japanese fintech firm is making it compuls...</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-14 19:40:29-04:00</td>\n",
       "      <td>An unlikely startup guru has emerged in Japan,...</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-14 19:35:41-04:00</td>\n",
       "      <td>Some US cities are late in making financial di...</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-14 19:31:07-04:00</td>\n",
       "      <td>The shipping industry is looking to rethink ev...</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-14 19:25:09-04:00</td>\n",
       "      <td>A biotech wants to cut fashion waste by using ...</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26809</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>Toymakers have found a new group of customers:...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26810</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>Belarusian hackers and dissidents determined t...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26811</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>Landlords are taking out millions in loans to ...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26812</th>\n",
       "      <td>2022-12-31 19:00:09-05:00</td>\n",
       "      <td>It took a pandemic to make a dent in US inequa...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26813</th>\n",
       "      <td>2022-12-31 19:00:08-05:00</td>\n",
       "      <td>A planned train line in Mexico is billions ove...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26814 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Time  \\\n",
       "0     2023-03-14 19:40:29-04:00   \n",
       "1     2023-03-14 19:40:29-04:00   \n",
       "2     2023-03-14 19:35:41-04:00   \n",
       "3     2023-03-14 19:31:07-04:00   \n",
       "4     2023-03-14 19:25:09-04:00   \n",
       "...                         ...   \n",
       "26809 2022-12-31 19:00:09-05:00   \n",
       "26810 2022-12-31 19:00:09-05:00   \n",
       "26811 2022-12-31 19:00:09-05:00   \n",
       "26812 2022-12-31 19:00:09-05:00   \n",
       "26813 2022-12-31 19:00:08-05:00   \n",
       "\n",
       "                                                   Tweet        Date  \n",
       "0      One Japanese fintech firm is making it compuls...  2023-03-14  \n",
       "1      An unlikely startup guru has emerged in Japan,...  2023-03-14  \n",
       "2      Some US cities are late in making financial di...  2023-03-14  \n",
       "3      The shipping industry is looking to rethink ev...  2023-03-14  \n",
       "4      A biotech wants to cut fashion waste by using ...  2023-03-14  \n",
       "...                                                  ...         ...  \n",
       "26809  Toymakers have found a new group of customers:...  2022-12-31  \n",
       "26810  Belarusian hackers and dissidents determined t...  2022-12-31  \n",
       "26811  Landlords are taking out millions in loans to ...  2022-12-31  \n",
       "26812  It took a pandemic to make a dent in US inequa...  2022-12-31  \n",
       "26813  A planned train line in Mexico is billions ove...  2022-12-31  \n",
       "\n",
       "[26814 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "import pytz\n",
    "\n",
    "business = []\n",
    "dates = []\n",
    "f = open('business3.txt', 'r', encoding='utf-8')\n",
    "\n",
    "for l in f:\n",
    "    line = l.split('~')\n",
    "    date_str = line[0]\n",
    "    try:\n",
    "        date_time = dt.fromisoformat(date_str)\n",
    "        date_time = date_time.astimezone(pytz.timezone(\"US/Eastern\"))\n",
    "        line[0] = date_time\n",
    "        line[1] = line[1][:-1]\n",
    "        business.append(line)\n",
    "        dates.append(date_time.date())\n",
    "    except:\n",
    "        business[-1][1] += \" \"+l[:-1]\n",
    "f.close()\n",
    "\n",
    "business = pd.DataFrame(business, columns=['Time', 'Tweet'])\n",
    "business['Date'] = dates\n",
    "\n",
    "business"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054e182",
   "metadata": {},
   "source": [
    "### Question 1.3\n",
    "Using your favorite sentiment analyzer (e.g., `vaderSentiment`), find the average sentiment for the headlines on each date that data was collected. Concatenate this sentiment score to your data frame of log returns. Use the `print` command to display your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a95df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "2023-01-03  384.369995  386.429993  377.829987  380.820007  379.372131   \n",
      "2023-01-04  383.179993  385.880005  380.000000  383.760010  382.300964   \n",
      "2023-01-05  381.720001  381.839996  378.760010  379.380005  377.937592   \n",
      "2023-01-06  382.609985  389.250000  379.410004  388.079987  386.604492   \n",
      "2023-01-09  390.369995  393.700012  387.670013  387.859985  386.385345   \n",
      "\n",
      "               Volume   log_ret      sent  \n",
      "Date                                       \n",
      "2023-01-03   74850700       NaN -0.007160  \n",
      "2023-01-04   85934100  0.007691 -0.010883  \n",
      "2023-01-05   76970500 -0.011479 -0.032084  \n",
      "2023-01-06  104189600  0.022673 -0.001990  \n",
      "2023-01-09   73978100 -0.000567  0.048762  \n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment = []\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for tweet in business.Tweet:\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    sentiment.append(vs[\"compound\"])\n",
    "\n",
    "business['Sentiment'] = sentiment\n",
    "\n",
    "data['sent'] = business.pivot_table(index='Date', values='Sentiment', aggfunc='mean')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-tiger",
   "metadata": {},
   "source": [
    "## Question 2 (20pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-lodging",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Linearly regress `SPY` returns as a function of the lagged returns (2 lags).\n",
    "This should be of the form $r_{t} = \\beta_0 + \\beta_{1} r_{t-1} + \\beta_{2} r_{t-2}$.\n",
    "Evaluate the performance of this model with the mean squared error of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fifteen-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for model 1: 0.00011083584269359484\n",
      "log_ret_1 : 0.05614851635772728\n",
      "log_ret_2 : -0.12133150745716827\n",
      "Intercept : 0.0007263457920817708\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.filterwarnings('ignore', category=SettingWithCopyWarning)\n",
    "\n",
    "\n",
    "df = data[['sent', 'Adj Close', 'log_ret']]\n",
    "\n",
    "for i in range(1, 3):\n",
    "    df[f\"log_ret_{i}\"] = df['log_ret'].shift(i)\n",
    "    df[f\"sent_{i}\"] = df['sent'].shift(i)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "y = df['log_ret']\n",
    "X = df[['log_ret_1', 'log_ret_2']]\n",
    "\n",
    "# X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.2, random_state=personal)\n",
    "\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X, y)\n",
    "\n",
    "y_pred_1 = model1.predict(X)\n",
    "mse_1 = mean_squared_error(y, y_pred_1)\n",
    "\n",
    "print(\"MSE for model 1:\", mse_1)\n",
    "\n",
    "for coef, var in zip(model1.coef_, X.columns):\n",
    "    print(f\"{var:>5} : {coef}\")\n",
    "\n",
    "print(f\"Intercept : {model1.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c35c32",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Linearly regress `SPY` returns as a function of the lagged sentiment (2 lags).\n",
    "This should be of the form $r_{t} = \\beta_0 + \\beta_{1} s_{t-1} + \\beta_{2} s_{t-2}$.\n",
    "Evaluate the performance of this model with the mean squared error of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d95eebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for model 2: 0.00010145279900828163\n",
      "   sent_1 : -0.06312613243734158\n",
      "   sent_2 : -0.07328922397704475\n",
      "Intercept : 0.0015461034838210735\n"
     ]
    }
   ],
   "source": [
    "y = df['log_ret']\n",
    "X = df[['sent_1', 'sent_2']]\n",
    "\n",
    "# X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.2, random_state=personal)\n",
    "\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X, y)\n",
    "\n",
    "y_pred_2 = model2.predict(X)\n",
    "mse_2 = mean_squared_error(y, y_pred_2)\n",
    "\n",
    "print(\"MSE for model 2:\", mse_2)\n",
    "\n",
    "for coef, var in zip(model2.coef_, X.columns):\n",
    "    print(f\"{var:>9} : {coef}\")\n",
    "\n",
    "print(f\"Intercept : {model2.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75084782",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "Linearly regress `SPY` returns as a function of the lagged returns and sentiment (2 lags each).\n",
    "This should be of the form $r_{t} = \\beta_0 + \\beta_{1,r} r_{t-1} + \\beta_{2,r} r_{t-2} + \\beta_{1,s} s_{t-1} + \\beta_{2,s} s_{t-2}$.\n",
    "Evaluate the performance of this model with the mean squared error of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cccc190f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for model 3: 0.00010082831355879292\n",
      "log_ret_1 : 0.030041028903574258\n",
      "log_ret_2 : -0.07178435657940134\n",
      "   sent_1 : -0.058105002975061194\n",
      "   sent_2 : -0.07250676753721168\n",
      "Intercept : 0.0015323903400410355\n"
     ]
    }
   ],
   "source": [
    "y = df['log_ret']\n",
    "X = df[['log_ret_1', 'log_ret_2', 'sent_1', 'sent_2']]\n",
    "\n",
    "# X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X, y, test_size=0.2, random_state=personal)\n",
    "\n",
    "model3 = LinearRegression()\n",
    "model3.fit(X, y)\n",
    "\n",
    "y_pred_3 = model3.predict(X)\n",
    "mse_3 = mean_squared_error(y, y_pred_3)\n",
    "\n",
    "print(\"MSE for model 3:\", mse_3)\n",
    "\n",
    "for coef, var in zip(model3.coef_, X.columns):\n",
    "    print(f\"{var:>9} : {coef}\")\n",
    "\n",
    "print(f\"Intercept : {model3.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b95dac",
   "metadata": {},
   "source": [
    "### Question 2.4\n",
    "Compare the performance of these 3 linear regressions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27f1d290",
   "metadata": {},
   "source": [
    "It seems like the linear regression does the best when it is just the sentiment being analyzed. That is where the MSE is the lowest and also that the lagged return is not as solid of a predictor as the sentiment. The MSE is slighly better when just sentiment, but it is close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b821272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for LASSO: 0.00010161782815104636\n",
      "log_ret_1 : 0.0\n",
      "log_ret_2 : -0.0\n",
      "   sent_1 : -0.05606639040958814\n",
      "   sent_2 : -0.06384601609841364\n",
      "Intercept : 0.0014422150856061768\n"
     ]
    }
   ],
   "source": [
    "y = df['log_ret']\n",
    "X = df[['log_ret_1', 'log_ret_2', 'sent_1', 'sent_2']]\n",
    "\n",
    "# X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(X, y, test_size=0.2, random_state=personal)\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.00001)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "y_pred_4 = lasso.predict(X)\n",
    "mse_4 = mean_squared_error(y, y_pred_4)\n",
    "\n",
    "print(\"MSE for LASSO:\", mse_4)\n",
    "\n",
    "for coef, var in zip(lasso.coef_, X.columns):\n",
    "    print(f\"{var:>9} : {coef}\")\n",
    "\n",
    "print(f\"Intercept : {lasso.intercept_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8186f683",
   "metadata": {},
   "source": [
    "When using LASSO, you can see that the log returns get penalized a lot more than sentiment, meaning that the algorithm also believes that sentiment is more important as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1ba48",
   "metadata": {},
   "source": [
    "## Question 3 (20pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e9780",
   "metadata": {},
   "source": [
    "### Question 3.1\n",
    "Regress `SPY` returns with a random forest as a function of the lagged returns (2 lags).\n",
    "This should be of the form $r_{t} = f(r_{t-1} , r_{t-2})$.\n",
    "Evaluate the performance of this model with the mean squared error of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72373175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Random Forest: 2.467667811997739e-05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def rf(X, y):\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=personal)\n",
    "    rf = RandomForestRegressor(random_state=personal)\n",
    "\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    y_pred = rf.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    print(\"MSE for Random Forest:\", mse)\n",
    "\n",
    "    return rf\n",
    "\n",
    "y = df['log_ret']\n",
    "X = df[['log_ret_1', 'log_ret_2']]\n",
    "\n",
    "rf1 = rf(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74636454",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "Regress `SPY` returns with a random forest as a function of the lagged sentiments (2 lags).\n",
    "This should be of the form $r_{t} = f(s_{t-1} , s_{t-2})$.\n",
    "Evaluate the performance of this model with the mean squared error of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d275427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Random Forest: 1.9185302109313623e-05\n"
     ]
    }
   ],
   "source": [
    "y = df['log_ret']\n",
    "X = df[['sent_1', 'sent_2']]\n",
    "\n",
    "rf2 = rf(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46acca7",
   "metadata": {},
   "source": [
    "### Question 3.3\n",
    "Regress `SPY` returns with a random forest as a function of the lagged returns and sentiment (2 lags each).\n",
    "This should be of the form $r_{t} = f(r_{t-1} , r_{t-2} , s_{t-1} , s_{t-2})$.\n",
    "Evaluate the performance of this model with the mean squared error of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd4619b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Random Forest: 2.0071930255675254e-05\n"
     ]
    }
   ],
   "source": [
    "y = df['log_ret']\n",
    "X = df[['log_ret_1', 'log_ret_2', 'sent_1', 'sent_2']]\n",
    "rf3 = rf(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85526ad8",
   "metadata": {},
   "source": [
    "### Question 3.4\n",
    "Compare the performance of these 3 random forest regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f01dcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_ret_1 : 0.15087334302566752\n",
      "log_ret_2 : 0.19491711197686978\n",
      "   sent_1 : 0.4154345731427369\n",
      "   sent_2 : 0.23877497185472585\n"
     ]
    }
   ],
   "source": [
    "for var, imp in zip(X.columns, rf3.feature_importances_):\n",
    "    print(f\"{var:>9} : {imp}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c19fc22c",
   "metadata": {},
   "source": [
    "The same thing happens here similar to the linear regression: The random forest does the best when it is just the sentiment being analyzed. The MSE is lowest and it seems like having the lagged returns makes it worse. The feature importances of the random forest also show a stronger importance of sentiment than the lagged returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af53fc9",
   "metadata": {},
   "source": [
    "## Question 4 (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a4d61",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Compare the performance of the various regressions utilized. Do you find the text data to be a useful feature in your analysis. Explain why or why not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "767e1622",
   "metadata": {},
   "source": [
    "Both of these regressions said the same thing: sentiment is the most important features in the model. Not only was the MSE the lowest in both regressions that only used sentiment, but also, LASSO dropped out the log returns and the random forest showed the sentiment feature importances were higher than log returns. For this reason, text data is a useful feature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
